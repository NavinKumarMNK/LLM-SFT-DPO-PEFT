sft:
  seed: 42
  data:
    path: /workspace/LLM/data/alpaca/
    params:
      max_len: 2048
      val_size: 0.2
      num_proc: 4
  model:
    path: /workspace/LLM/models/mistral-7b/
    params:
      quantization: 4-bit
      torch_dtype: auto
      quantization_config:
        use_double_quant: True,
        quant_type: nf4
      peft_config:
        model_path: False  # [path, False]
        target_modules: [all]
        modules_to_save: [all]
        r: 1
        alpha: 0.5
        dropout: 0.2
  trainer:
    params:
      batch_size: 8


     